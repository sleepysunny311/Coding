{"cells":[{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import binom\n","from scipy.stats import beta\n","import re\n","from scipy.optimize import minimize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVmwdwFLrJKO"},"outputs":[],"source":["# The following is a simple example of how to use the ELBO function basing on https://rstudio-pubs-static.s3.amazonaws.com/630126_c2063706da044eb6a2944d7027dcce4d.html\n","# There is open source code for the ELBO function using python at https://pyro.ai/examples/svi_part_i.html"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def log_likelihood(x,z):\n","    # x is a vector of observed data\n","    # z is a vector of latent variables\n","    # returns the log likelihood of the data given the latent variables\n","    # in this case, the log likelihood is the log probability mass function of a binomial distribution\n","    # The pruned binomial distribution is used to avoid numerical issues\n","\n","    val = binom.logpmf(x, 1, z)\n","    val[np.isinf(val) & (val < 0)] = -10000\n","    val[np.isinf(val) & (val > 0)] = 10000\n","    return val"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def log_prior(z):\n","    # z is a vector of latent variables\n","    # returns the log prior of the latent variables\n","    # in this case, the log prior is the log probability density function of a beta distribution\n","    # The pruned beta distribution is used to avoid numerical issues\n","    # The initial values of the beta distribution are set to 10,10\n","    val = beta.logpdf(z, a=10, b=10)\n","    val[np.isinf(val) & (val < 0)] = -10000\n","    val[np.isinf(val) & (val > 0)] = 10000\n","    return val"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def log_variational(z, phi):\n","    # z is a vector of latent variables\n","    # phi is a vector of variational parameters\n","    # returns the log variational distribution of the latent variables\n","    # in this case, the log variational distribution is the log probability density function of a beta distribution\n","    # The pruned beta distribution is used to avoid numerical issues\n","    val = beta.logpdf(z, a=phi[0], b=phi[1])\n","    val[np.isinf(val) & (val < 0)] = -10000\n","    val[np.isinf(val) & (val > 0)] = 10000\n","    return val"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["## FIXME:: We can vectorize this functiont to speed up the process\n","\n","def ELBO(phi, x, n_samples=10000):\n","    # phi is a vector of variational parameters\n","    # x is a vector of observed data\n","    # n_samples is the number of samples used to approximate the expectation\n","    # returns the simulated ELBO value\n","    np.random.seed(20230125)\n","    sum = 0\n","    for i in range(n_samples):\n","        # simulates z\n","        z = beta.rvs(a=phi[0], b=phi[1], size=1)\n","\n","        # sum of log likelihoods\n","        sum_log_lik = np.sum(log_likelihood(x, z))\n","\n","        # log prior\n","        log_pr = log_prior(z)\n","\n","        # log variational\n","        log_var = log_variational(z, phi)\n","        sum += sum_log_lik + log_pr - log_var\n","\n","    # average (approximates expectation)\n","    return sum / n_samples\n","def neg_ELBO(phi, x, n_samples=10000):\n","    # phi is a vector of variational parameters\n","    # x is a vector of observed data\n","    # n_samples is the number of samples used to approximate the expectation\n","    # returns the negative ELBO value\n","    return -ELBO(phi, x, n_samples)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def optim_ELBO(phi0, x_sample, n_samples):\n","    # fnscale -1 is used for specifying a maximization problem\n","    bounds = [(0.00001, 100), (0.00001, 100)]\n","    result = minimize(neg_ELBO, phi0, args=(x_sample, n_samples), bounds=bounds, method='L-BFGS-B')\n","    return result"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def plot_posterior(phi_opt, true_prob, n_tos,plot_ax):\n","    x = np.linspace(0, 1, 100)\n","    y = beta.pdf(x, a=phi_opt[0], b=phi_opt[1])\n","    plot_ax.plot(x, y, 'r-', lw=2, label='Posterior')\n","    plot_ax.axvline(true_prob, color='blue', lw=1.6, label='True Probability')\n","    plot_ax.set_xlabel('z')\n","    plot_ax.set_ylabel('q(z)')\n","    plot_ax.set_title(f'With {n_tos} tosses')\n","    plot_ax.legend()"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["\n","#True probability of heads\n","true_prob = 0.6\n","\n","#Initial values for alpha and beta\n","phi0 = [8, 5]\n","\n","#Number of samples from \n","#the variational distribution\n","n_samples = 2000\n","\n","#Number of tosses\n","n_tos = 100\n","\n","fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n","axs = axs.ravel()\n","to_plot = []\n","for i, ax in zip(range(20, 140, 20), axs):\n","    x_sample = np.random.binomial(1, true_prob, size=i)\n","\n","    phi_opt = optim_ELBO(phi0, x_sample, n_samples)\n","\n","    plot_posterior(phi_opt.x, true_prob, i, ax)\n","plt.show()\n","\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["k4PiTTR3596s","JModcw-EYP-8","mTL04Qb6s3gl","m6XQ_uwmj270","EalYYwuihjrj","L0b2tQkLlk0l","ubzfLpYeeLpS"],"provenance":[{"file_id":"1Ce5Sg0Gcu9-rPPmXa6WOE-F2x_mI9bxc","timestamp":1670528387167}]},"kernelspec":{"display_name":"GR5206","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"067f566133fb7f8433f22dbf639f7e4fdb2f6ce37577fcdb562a4ab9c2b1cd0a"}}},"nbformat":4,"nbformat_minor":0}
